{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Draft\n",
    "\n",
    "Our current president loves to talk about climate change. While President Trump's tweets have lately been more focused on what might consider to be more pressing matters for his administration (No collusion!), Trump has publicly expressed his opinions on climate science no less than 145 times in the past 6 years. \n",
    "\n",
    "If you take a look at the content of these tweets, you'll find two key themes.\n",
    "\n",
    "__1. Climate change / global warming is a conspiracy.__ President Trump is well known for his stance on climate change, notably backing out of the Paris Climate Accord early in his first term. Public opinion on this message is divisive--President Trump's supporters seem to echo this sentiment, and his opponents often use as fodder for political attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for what to do going forward:\n",
    "\n",
    "1. Separate notebooks into two separate chunks (introduction / interest trends)\n",
    "2. Exploring text data\n",
    "3. Building classifier\n",
    "\n",
    "For sections 1 and two, provide clearer discussion and annotation for functions.\n",
    "\n",
    "Add headings for different subsections, provide justification for each question or subquestion that is asked.\n",
    "\n",
    "Make sure the introduction isn't too politically motivated (try to be as objective as possible)\n",
    "\n",
    "Note where I got code chunks for tweets, etc.\n",
    "\n",
    "Assemble corpus of text from titles\n",
    "\n",
    "- Clean up corpus (follow steps from current homework)\n",
    "- Construct word clouds for different labels\n",
    "- Remove labels and produce classifier that predicts label used\n",
    "- Also check whether the specific subreddit matters (perhaps look at whether you can predict whether text came from climatechange or climateskeptics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">\n",
       "The concept of global warming was created by and for the Chinese in order to make U.S. \n",
       "manufacturing non-competitive.</p>&mdash; Donald J. Trump (@realDonaldTrump) \n",
       "<a href=\"https://twitter.com/realDonaldTrump/status/265895292191248385?ref_src=twsrc%5Etfw\">November 6, 2012</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<__main__.Tweet at 0x1b3c298dd68>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tweet(object):\n",
    "    def __init__(self, embed_str=None):\n",
    "        self.embed_str = embed_str\n",
    "    def _repr_html_(self):\n",
    "        return self.embed_str\n",
    "    \n",
    "address = (\"\"\"\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">\n",
    "The concept of global warming was created by and for the Chinese in order to make U.S. \n",
    "manufacturing non-competitive.</p>&mdash; Donald J. Trump (@realDonaldTrump) \n",
    "<a href=\"https://twitter.com/realDonaldTrump/status/265895292191248385?ref_src=twsrc%5Etfw\">November 6, 2012</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\"\"\")\n",
    "\n",
    "Tweet(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Current temperatures provide evidence for this fact:__ President Trump also likes to draw attention to cold weather patterns, using them to justify his attacks on climate science.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">\n",
       "In the East, it could be the COLDEST New Year’s Eve on record. \n",
       "Perhaps we could use a little bit of that good old Global Warming that our Country, \n",
       "but not other countries, was going to pay TRILLIONS OF DOLLARS to protect against. Bundle up!\n",
       "</p>&mdash; Donald J. Trump (@realDonaldTrump) \n",
       "<a href=\"https://twitter.com/realDonaldTrump/status/946531657229701120?ref_src=twsrc%5Etfw\">\n",
       "December 29, 2017</a></blockquote>\n",
       "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<__main__.Tweet at 0x1b3c29b0438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = (\"\"\"\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">\n",
    "In the East, it could be the COLDEST New Year’s Eve on record. \n",
    "Perhaps we could use a little bit of that good old Global Warming that our Country, \n",
    "but not other countries, was going to pay TRILLIONS OF DOLLARS to protect against. Bundle up!\n",
    "</p>&mdash; Donald J. Trump (@realDonaldTrump) \n",
    "<a href=\"https://twitter.com/realDonaldTrump/status/946531657229701120?ref_src=twsrc%5Etfw\">\n",
    "December 29, 2017</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\"\"\")\n",
    "\n",
    "Tweet(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an ecologist, this pattern of behavior is troubling -- while I welcome any discussion of how to best address climate change through legislation or commercial activity, climate science is subject to rigorous peer review and its underlying causes are well understood. \n",
    "\n",
    "One might describe President Trump's actions as a form of recency bias or \"ecological amnesia\", a problem that many of us in environmental science grapple with when communicating research. Many ecological phenomena occur over long timescales that are slower to change than our perceptions of what is \"normal\". Climate trends, for example, can only be detected with 100+ year observations, rather than the short periods that form our frame of reference. As humans, it's much easier for us to focus on outliers than to detect a slow, gradual change.\n",
    "\n",
    "Case in point; many of President Trump's tweets on Climate change occur in winter months when temperatures *feel* cold, even if they fall within normal ranges of variation. \n",
    "\n",
    "He also likes to use the term \"global warming\", which has fallen out of use among scientists, in large part because it poorly represents the effects of climate change. True, the climate is getting warmer on average, but experts often place greater emphasis on increased variability in temperature and precipitation.\n",
    "\n",
    "Together, recency bias and poor nomenclature may form a substantial barrier to public understanding of global climate change. In this project, I aim to explore how prevalent these same patterns in the general public.\n",
    "\n",
    "__Hypotheses:__\n",
    "\n",
    "1. US Google searches for the terms \"climate change\" and \"global warming\" will peak annually in winter months.\n",
    "\n",
    "2. Use of the terms \"climate change\" and \"global warming\" correlate with political affiliation. User activity in social media forums more closely linked to the current presidential administration will use the term \"global warming\", while communities with stronger ties to Democratic politics will use the term \"climate change\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning:\n",
      "\n",
      "The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ebatzer/242.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import statsmodels\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import statsmodels.api as sm\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "\n",
    "plotly.tools.set_credentials_file(username='ebatzer', api_key='BIJOCGPZqKooZ16thhcw')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "gtrends = pd.read_csv(\"googletrends.csv\")\n",
    "gtrends.columns = [\"month\", \"climate change\", \"global warming\"]\n",
    "gtrends[\"month\"] = gtrends[\"month\"] + \"/01\"\n",
    "gtrends[\"date_num\"] = pd.to_datetime(gtrends[\"month\"], yearfirst = True)\n",
    "gtrends[\"total\"] = gtrends[\"climate change\"] + gtrends[\"global warming\"]\n",
    "gtrends[\"total\"] = (gtrends[\"total\"] / max(gtrends[\"total\"])) * 100\n",
    "\n",
    "# Create traces\n",
    "data = Data([\n",
    "    Scatter(\n",
    "        y = gtrends[\"global warming\"],\n",
    "        x = gtrends[\"date_num\"],\n",
    "        marker=Line(\n",
    "            color = \"red\"\n",
    "        ),\n",
    "        mode='lines',\n",
    "        name = 'Global Warming',\n",
    "        showlegend = True),\n",
    "    Scatter(\n",
    "        y = gtrends[\"climate change\"],\n",
    "        x = gtrends[\"date_num\"],\n",
    "        marker=Line(\n",
    "            color = \"blue\"\n",
    "        ),\n",
    "        mode='lines',\n",
    "        name = 'Climate Change',\n",
    "        showlegend = True)\n",
    "])\n",
    "\n",
    "layout = Layout(\n",
    "    title='Google Searches for \"Climate Change\" and \"Global Warming\"',\n",
    "    xaxis=dict(\n",
    "        title='Date',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        ),\n",
    "        showgrid=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Frequency',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        ),\n",
    "        showgrid=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ebatzer/244.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_csv(\"NOAAweatherdata.csv\", skiprows = 4) \n",
    "weather[\"Month\"] = weather[\"Date\"] % 100\n",
    "weather[\"Year\"] = weather[\"Date\"].apply(str).apply(lambda x: x[:4])\n",
    "weather[\"Date\"] = weather[\"Year\"] + \"/\" + weather[\"Month\"].apply(str) + \"/01\"\n",
    "weather[\"date_num\"] = pd.to_datetime(weather[\"Date\"], yearfirst = True)\n",
    "gtrends = pd.merge(gtrends, weather, \"inner\")\n",
    "\n",
    "lowess = sm.nonparametric.lowess\n",
    "ys = lowess( gtrends[\"total\"], pd.to_numeric(gtrends[\"date_num\"]), frac = .3)[:,1]\n",
    "gtrends['polyfit'] = ys\n",
    "gtrends['resid'] = gtrends[\"total\"] - gtrends['polyfit']\n",
    "\n",
    "# Create traces\n",
    "data = Data([\n",
    "    Scatter(\n",
    "        y = gtrends[\"total\"],\n",
    "        x = gtrends[\"date_num\"],\n",
    "        marker=Marker(\n",
    "            size=12,\n",
    "            cmax=80,\n",
    "            cmin=30,\n",
    "            color= gtrends[\"Value\"],\n",
    "            colorbar=ColorBar(\n",
    "                title='Mean US Temperature'\n",
    "            ),\n",
    "            colorscale='Viridis'\n",
    "        ),\n",
    "        mode='lines+markers',\n",
    "        showlegend = False),\n",
    "    Scatter(\n",
    "        y= gtrends[\"polyfit\"],\n",
    "        x = gtrends[\"date_num\"],\n",
    "        line=Line(\n",
    "            color= \"black\"),\n",
    "        mode='lines',\n",
    "        showlegend = False),\n",
    "])\n",
    "\n",
    "layout = Layout(\n",
    "    title='Google Searches for \"Climate Change\" and \"Global Warming\"',\n",
    "    xaxis=dict(\n",
    "        title='Date',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'),\n",
    "        showgrid=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Frequency',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'),\n",
    "        showgrid=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts on why there is a peak in searches ca. 2007 -\n",
    "\n",
    "- \"An Inconvenient Truth\" released in 2006 - wins Academy Award for best documentary feature (February 2007), Al Gore wins Nobel Peace Prize (October 2007).\n",
    "- IPCC report published in 2007 (fourth since 1990, most substantial and comprehensize undertaken)\n",
    "- Skeptical science - Global warming stopped in 2007? Cold climate conditions across the US (also in 2002, 2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gore_lines = [\"2006-05-24\",\"2007-02-26\",\"2007-10-12\"]\n",
    "labels = ['\"An Incovenient Truth\" Released', 'AIT Wins Best Documentary', 'Al Gore and IPCC win Nobel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ebatzer/328.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create traces\n",
    "data = Data([\n",
    "            Scatter(\n",
    "                y = gtrends[\"total\"],\n",
    "                x = gtrends[\"date_num\"],\n",
    "                marker=Marker(\n",
    "                    size=12,\n",
    "                    cmax=80,\n",
    "                    cmin=30,\n",
    "                    color= gtrends[\"Value\"],\n",
    "                    colorbar=ColorBar(\n",
    "                        title='Mean US Temperature'\n",
    "                    ),\n",
    "                    colorscale='Viridis'\n",
    "                ),\n",
    "                mode='lines+markers',\n",
    "                showlegend = False),\n",
    "            Scatter(\n",
    "                y= gtrends[\"polyfit\"],\n",
    "                x = gtrends[\"date_num\"],\n",
    "                line=Line(\n",
    "                    color= \"black\"),\n",
    "                mode='lines',\n",
    "                showlegend = False),\n",
    "            Scatter(\n",
    "                y = [0, 100],\n",
    "                x = [gore_lines[0], gore_lines[0]],\n",
    "                mode = \"lines\",\n",
    "                line = Line(color = \"red\"),\n",
    "                showlegend = False),\n",
    "            Scatter(\n",
    "                y = [0, 100],\n",
    "                x = [gore_lines[1], gore_lines[1]],\n",
    "                mode = \"lines\",\n",
    "                line = Line(color = \"red\"),\n",
    "                showlegend = False),\n",
    "            Scatter(\n",
    "                y = [0, 100],\n",
    "                x = [gore_lines[2], gore_lines[2]],\n",
    "                mode = \"lines\",\n",
    "                line = Line(color = \"red\"),\n",
    "                showlegend = False)\n",
    "])\n",
    "\n",
    "annotations = []\n",
    "\n",
    "for label, xval, yadj in zip(labels, gore_lines, [0, 20, 40]):\n",
    "    annotations.append(dict( x=xval, y=100, ay = -yadj,\n",
    "                                      xanchor='right', yanchor='middle',\n",
    "                                      text=label,\n",
    "                                      font=dict(family='Arial',\n",
    "                                                size=16),\n",
    "                                      showarrow=True))\n",
    "    \n",
    "layout[\"annotations\"] = annotations\n",
    "fig = Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ebatzer/246.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.poly1d(np.polyfit(gtrends[\"Value\"], gtrends[\"resid\"], 2))\n",
    "predseq = np.linspace(30,80).tolist()\n",
    "quadfit = f(predseq)\n",
    "\n",
    "# Create traces\n",
    "data = Data([\n",
    "    Scatter(\n",
    "        y= gtrends[\"resid\"],\n",
    "        x= gtrends[\"Value\"],\n",
    "        marker=Marker(\n",
    "            size=12,\n",
    "            cmax=80,\n",
    "            cmin=0,\n",
    "            color=  gtrends[\"Value\"],\n",
    "            colorbar=ColorBar(\n",
    "                title='Temperature (F)'\n",
    "            ),\n",
    "            colorscale='Viridis'\n",
    "        ),\n",
    "        mode='markers',\n",
    "        showlegend = False),\n",
    "    Scatter(\n",
    "        y = quadfit,\n",
    "        x = predseq,\n",
    "        line=Line(\n",
    "            color= \"black\"),\n",
    "        mode='lines',\n",
    "        showlegend = False)\n",
    "])\n",
    "\n",
    "\n",
    "layout = Layout(\n",
    "    title='LOWESS Residuals',\n",
    "    xaxis=dict(\n",
    "        title='Mean US Temperature',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        ),\n",
    "        showgrid=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Residuals',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        ),\n",
    "        showgrid=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "import time\n",
    "\n",
    "requests_cache.install_cache('reddit_cache')\n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "\n",
    "# Sets dates - will run for all days between start and end\n",
    "dstart = datetime.date(2008, 1, 1)\n",
    "dend = datetime.date(2018, 1, 1)\n",
    "steps = (dend - dstart).days\n",
    "\n",
    "# Sets timesteps (start date, the first timestep, and the size between them, in epochs)\n",
    "t0 = pd.to_numeric(time.mktime(time.strptime(\"2008/01/01 00:00:00 GMT\", \"%Y/%m/%d %H:%M:%S %Z\")))\n",
    "t1 = pd.to_numeric(time.mktime(time.strptime(\"2018/01/01 00:00:00 GMT\", \"%Y/%m/%d %H:%M:%S %Z\")))\n",
    "\n",
    "# Define the search subs function:\n",
    "def search_subs(url, searchterm, sub, size, after, before):\n",
    "    \n",
    "    # Requests PushShift API\n",
    "    req = requests.get(url, params = {\"title\" : searchterm,\n",
    "                                     \"after\": after,\n",
    "                                     \"before\": before,\n",
    "                                     \"subreddit\": sub,\n",
    "                                     \"size\": size})\n",
    "\n",
    "    # Selects data element\n",
    "    subdata = pd.DataFrame(req.json()['data'])\n",
    "    \n",
    "    # Adds identifying columns\n",
    "    subdata[\"searchstart\"] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(after))\n",
    "    subdata[\"searchend\"] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(before))\n",
    "    subdata[\"keyword\"] = searchterm\n",
    "    \n",
    "    return(subdata, subdata[\"created_utc\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('sqlite:///subs.db')\n",
    "\n",
    "# search_utc = int(t0)\n",
    "# outputdf = dict()\n",
    "# count = 1\n",
    "\n",
    "# while search_utc < int(t1) :\n",
    "#     subs, search_utc = search_subs( \n",
    "#             url = url, \n",
    "#             searchterm = \"climate change\", \n",
    "#             sub = None, \n",
    "#             after = search_utc,\n",
    "#             before = None,\n",
    "#             size = 500)\n",
    "    \n",
    "#     cols = [\"author\", \"created_utc\", \"domain\", \"id\", \"num_comments\",\n",
    "#            \"permalink\", \"score\", \"subreddit\", \"subreddit_id\",\n",
    "#            \"title\", \"url\", \"searchstart\", \"searchend\", \"keyword\"]\n",
    "#     subs = subs[cols]\n",
    "#     subs.to_sql(\"cctable\", engine, if_exists = \"append\")\n",
    "    \n",
    "#     search_utc = search_utc + 1\n",
    "\n",
    "#     if count % 25 == 0:\n",
    "#         print(\"Current time is %s\" % time.strftime('%Y-%m-%d %H:%M:%S',\n",
    "#                                                    time.localtime(search_utc)))\n",
    "        \n",
    "#     count = count + 1\n",
    "      \n",
    "#     # Sleep to avoid over-requesting\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search_utc = int(t0)\n",
    "# outputdf = dict()\n",
    "# count = 1\n",
    "\n",
    "# while search_utc < int(t1) :\n",
    "#     subs, search_utc = search_subs( \n",
    "#             url = url, \n",
    "#             searchterm = \"global warming\", \n",
    "#             sub = None, \n",
    "#             after = search_utc,\n",
    "#             before = None,\n",
    "#             size = 500)\n",
    "    \n",
    "#     cols = [\"author\", \"created_utc\", \"domain\", \"id\", \"num_comments\",\n",
    "#            \"permalink\", \"score\", \"subreddit\", \"subreddit_id\",\n",
    "#            \"title\", \"url\", \"searchstart\", \"searchend\", \"keyword\"]\n",
    "#     subs = subs[cols]\n",
    "#     subs.to_sql(\"gwtable\", engine, if_exists = \"append\")\n",
    "    \n",
    "#     search_utc = search_utc + 1\n",
    "\n",
    "#     if count % 25 == 0:\n",
    "#         print(\"Current time is %s\" % time.strftime('%Y-%m-%d %H:%M:%S',\n",
    "#                                                    time.localtime(search_utc)))\n",
    "        \n",
    "#     count = count + 1\n",
    "      \n",
    "#     # Sleep to avoid over-requesting\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gwcounts = pd.read_sql(\"\"\"SELECT subreddit, COUNT(subreddit) \n",
    "FROM gwtable\n",
    "GROUP BY subreddit\n",
    "ORDER BY COUNT(subreddit) DESC\n",
    "\"\"\",\n",
    "               con = engine)\n",
    "\n",
    "cccounts = pd.read_sql(\"\"\"SELECT subreddit, COUNT(subreddit) \n",
    "FROM cctable\n",
    "GROUP BY subreddit\n",
    "ORDER BY COUNT(subreddit) DESC\n",
    "\"\"\",\n",
    "               con = engine)\n",
    "\n",
    "authors = pd.read_sql(\"\"\"SELECT subreddit, COUNT(DISTINCT author) \n",
    "FROM gwtable\n",
    "GROUP BY subreddit\n",
    "ORDER BY COUNT(DISTINCT author) DESC\n",
    "\"\"\",\n",
    "               con = engine)\n",
    "\n",
    "#################################\n",
    "# Some basic summary statistics #\n",
    "#################################\n",
    "\n",
    "gwcounts.columns = ['subreddit', 'gw_counts']\n",
    "cccounts.columns = ['subreddit', 'cc_counts']\n",
    "authors.columns = ['subreddit', 'authors']\n",
    "\n",
    "keywords = pd.merge(gwcounts, cccounts)\n",
    "keywords = pd.merge(keywords, authors)\n",
    "\n",
    "keywords[\"total\"] = keywords[\"cc_counts\"] + keywords[\"gw_counts\"]\n",
    "keywords[\"gw_frac\"] = keywords[\"gw_counts\"] / keywords[\"total\"]\n",
    "keywords[\"cc_frac\"] = keywords[\"cc_counts\"] / keywords[\"total\"]\n",
    "keywords[\"aut_frac\"] = keywords[\"authors\"] / keywords[\"total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which subreddits talk the most about climate change and global warming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]  [ (1,3) x3,y3 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ebatzer/248.embed\" height=\"600px\" width=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = Scatter(\n",
    "    x=keywords.sort_values(\"total\", ascending = False)[\"subreddit\"],\n",
    "    y=np.log(keywords.sort_values(\"total\", ascending = False)[\"total\"])\n",
    ")\n",
    "\n",
    "trace2 = Scatter(\n",
    "    x=keywords.sort_values(\"cc_counts\", ascending = False)[\"subreddit\"],\n",
    "    y=np.log(keywords.sort_values(\"cc_counts\", ascending = False)[\"cc_counts\"])\n",
    ")\n",
    "\n",
    "trace3 = Scatter(\n",
    "    x=keywords.sort_values(\"gw_counts\", ascending = False)[\"subreddit\"],\n",
    "    y=np.log(keywords.sort_values(\"gw_counts\", ascending = False)[\"gw_counts\"])\n",
    ")\n",
    "\n",
    "fig = plotly.tools.make_subplots(rows=1, cols=3)\n",
    "\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.append_trace(trace3, 1, 3)\n",
    "\n",
    "\n",
    "fig['layout'].update(height=600, width=600, title='Total keyword mentions (log scale)')\n",
    "py.iplot(fig, filename='simple-subplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "C:\\Users\\ebatz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ebatzer/278.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create traces\n",
    "data = Data([\n",
    "    Scatter(\n",
    "        y= np.log(keywords.sort_values(\"total\", ascending = False)[\"total\"]),\n",
    "        x= np.log(keywords.sort_values(\"total\", ascending = False)[\"authors\"]),\n",
    "        marker=Marker(\n",
    "            size= np.log(keywords.sort_values(\"total\", ascending = False)[\"total\"]) * 2,\n",
    "            cmax=1,\n",
    "            cmin=0,\n",
    "            color= keywords.sort_values(\"total\", ascending = False)[\"cc_frac\"],\n",
    "            colorbar=ColorBar(\n",
    "                title='Climate Change Fraction'\n",
    "            ),\n",
    "            colorscale='Viridis'\n",
    "        ),\n",
    "        mode='markers',\n",
    "        name='Markers',\n",
    "        text=keywords.sort_values(\"total\", ascending = False)[\"subreddit\"],\n",
    "        textposition='bottom',\n",
    "        showlegend = False)\n",
    "])\n",
    "\n",
    "\n",
    "layout = Layout(\n",
    "    title='Activity vs. Unique Number of Authors',\n",
    "    xaxis=dict(\n",
    "        title='Total Unique Authors (log scale)',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        ),\n",
    "        showgrid=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Total Number of Posts (log scale)',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        ),\n",
    "        showgrid=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>gw_counts</th>\n",
       "      <th>cc_counts</th>\n",
       "      <th>authors</th>\n",
       "      <th>total</th>\n",
       "      <th>gw_frac</th>\n",
       "      <th>cc_frac</th>\n",
       "      <th>aut_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>environment</td>\n",
       "      <td>6876</td>\n",
       "      <td>22549</td>\n",
       "      <td>2109</td>\n",
       "      <td>29425</td>\n",
       "      <td>0.233679</td>\n",
       "      <td>0.766321</td>\n",
       "      <td>0.071674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EcoInternet</td>\n",
       "      <td>1586</td>\n",
       "      <td>17293</td>\n",
       "      <td>2</td>\n",
       "      <td>18879</td>\n",
       "      <td>0.084009</td>\n",
       "      <td>0.915991</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>4343</td>\n",
       "      <td>12488</td>\n",
       "      <td>1800</td>\n",
       "      <td>16831</td>\n",
       "      <td>0.258036</td>\n",
       "      <td>0.741964</td>\n",
       "      <td>0.106946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>climate</td>\n",
       "      <td>2357</td>\n",
       "      <td>9978</td>\n",
       "      <td>469</td>\n",
       "      <td>12335</td>\n",
       "      <td>0.191082</td>\n",
       "      <td>0.808918</td>\n",
       "      <td>0.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reddit.com</td>\n",
       "      <td>4148</td>\n",
       "      <td>7340</td>\n",
       "      <td>2316</td>\n",
       "      <td>11488</td>\n",
       "      <td>0.361072</td>\n",
       "      <td>0.638928</td>\n",
       "      <td>0.201602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>4047</td>\n",
       "      <td>6955</td>\n",
       "      <td>1981</td>\n",
       "      <td>11002</td>\n",
       "      <td>0.367842</td>\n",
       "      <td>0.632158</td>\n",
       "      <td>0.180058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>4374</td>\n",
       "      <td>4626</td>\n",
       "      <td>526</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.058444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>2046</td>\n",
       "      <td>6026</td>\n",
       "      <td>960</td>\n",
       "      <td>8072</td>\n",
       "      <td>0.253469</td>\n",
       "      <td>0.746531</td>\n",
       "      <td>0.118930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>POLITIC</td>\n",
       "      <td>2072</td>\n",
       "      <td>4916</td>\n",
       "      <td>36</td>\n",
       "      <td>6988</td>\n",
       "      <td>0.296508</td>\n",
       "      <td>0.703492</td>\n",
       "      <td>0.005152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2811</td>\n",
       "      <td>4107</td>\n",
       "      <td>1784</td>\n",
       "      <td>6918</td>\n",
       "      <td>0.406331</td>\n",
       "      <td>0.593669</td>\n",
       "      <td>0.257878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>askscience</td>\n",
       "      <td>2887</td>\n",
       "      <td>3190</td>\n",
       "      <td>1958</td>\n",
       "      <td>6077</td>\n",
       "      <td>0.475070</td>\n",
       "      <td>0.524930</td>\n",
       "      <td>0.322198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>news</td>\n",
       "      <td>1486</td>\n",
       "      <td>4168</td>\n",
       "      <td>779</td>\n",
       "      <td>5654</td>\n",
       "      <td>0.262823</td>\n",
       "      <td>0.737177</td>\n",
       "      <td>0.137779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>2051</td>\n",
       "      <td>3501</td>\n",
       "      <td>1064</td>\n",
       "      <td>5552</td>\n",
       "      <td>0.369416</td>\n",
       "      <td>0.630584</td>\n",
       "      <td>0.191643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AutoNewspaper</td>\n",
       "      <td>667</td>\n",
       "      <td>4669</td>\n",
       "      <td>1</td>\n",
       "      <td>5336</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>1897</td>\n",
       "      <td>1644</td>\n",
       "      <td>1340</td>\n",
       "      <td>3541</td>\n",
       "      <td>0.535724</td>\n",
       "      <td>0.464276</td>\n",
       "      <td>0.378424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>1338</td>\n",
       "      <td>1462</td>\n",
       "      <td>512</td>\n",
       "      <td>2800</td>\n",
       "      <td>0.477857</td>\n",
       "      <td>0.522143</td>\n",
       "      <td>0.182857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>collapse</td>\n",
       "      <td>512</td>\n",
       "      <td>1904</td>\n",
       "      <td>172</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.788079</td>\n",
       "      <td>0.071192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>1090</td>\n",
       "      <td>1226</td>\n",
       "      <td>785</td>\n",
       "      <td>2316</td>\n",
       "      <td>0.470639</td>\n",
       "      <td>0.529361</td>\n",
       "      <td>0.338946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>383</td>\n",
       "      <td>1884</td>\n",
       "      <td>190</td>\n",
       "      <td>2267</td>\n",
       "      <td>0.168946</td>\n",
       "      <td>0.831054</td>\n",
       "      <td>0.083811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>autotldr</td>\n",
       "      <td>425</td>\n",
       "      <td>1790</td>\n",
       "      <td>1</td>\n",
       "      <td>2215</td>\n",
       "      <td>0.191874</td>\n",
       "      <td>0.808126</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit  gw_counts  cc_counts  authors  total   gw_frac  \\\n",
       "0         environment       6876      22549     2109  29425  0.233679   \n",
       "12        EcoInternet       1586      17293        2  18879  0.084009   \n",
       "2            politics       4343      12488     1800  16831  0.258036   \n",
       "7             climate       2357       9978      469  12335  0.191082   \n",
       "3          reddit.com       4148       7340     2316  11488  0.361072   \n",
       "4             science       4047       6955     1981  11002  0.367842   \n",
       "1     climateskeptics       4374       4626      526   9000  0.486000   \n",
       "10          worldnews       2046       6026      960   8072  0.253469   \n",
       "8             POLITIC       2072       4916       36   6988  0.296508   \n",
       "6           AskReddit       2811       4107     1784   6918  0.406331   \n",
       "5          askscience       2887       3190     1958   6077  0.475070   \n",
       "13               news       1486       4168      779   5654  0.262823   \n",
       "9          The_Donald       2051       3501     1064   5552  0.369416   \n",
       "21      AutoNewspaper        667       4669        1   5336  0.125000   \n",
       "11     Showerthoughts       1897       1644     1340   3541  0.535724   \n",
       "15         conspiracy       1338       1462      512   2800  0.477857   \n",
       "27           collapse        512       1904      172   2416  0.211921   \n",
       "16  explainlikeimfive       1090       1226      785   2316  0.470639   \n",
       "31      climatechange        383       1884      190   2267  0.168946   \n",
       "28           autotldr        425       1790        1   2215  0.191874   \n",
       "\n",
       "     cc_frac  aut_frac  \n",
       "0   0.766321  0.071674  \n",
       "12  0.915991  0.000106  \n",
       "2   0.741964  0.106946  \n",
       "7   0.808918  0.038022  \n",
       "3   0.638928  0.201602  \n",
       "4   0.632158  0.180058  \n",
       "1   0.514000  0.058444  \n",
       "10  0.746531  0.118930  \n",
       "8   0.703492  0.005152  \n",
       "6   0.593669  0.257878  \n",
       "5   0.524930  0.322198  \n",
       "13  0.737177  0.137779  \n",
       "9   0.630584  0.191643  \n",
       "21  0.875000  0.000187  \n",
       "11  0.464276  0.378424  \n",
       "15  0.522143  0.182857  \n",
       "27  0.788079  0.071192  \n",
       "16  0.529361  0.338946  \n",
       "31  0.831054  0.083811  \n",
       "28  0.808126  0.000451  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.sort_values(\"total\", ascending = False).iloc[:20,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>gw_counts</th>\n",
       "      <th>cc_counts</th>\n",
       "      <th>authors</th>\n",
       "      <th>total</th>\n",
       "      <th>gw_frac</th>\n",
       "      <th>cc_frac</th>\n",
       "      <th>aut_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>383</td>\n",
       "      <td>1884</td>\n",
       "      <td>190</td>\n",
       "      <td>2267</td>\n",
       "      <td>0.168946</td>\n",
       "      <td>0.831054</td>\n",
       "      <td>0.083811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>climate</td>\n",
       "      <td>2357</td>\n",
       "      <td>9978</td>\n",
       "      <td>469</td>\n",
       "      <td>12335</td>\n",
       "      <td>0.191082</td>\n",
       "      <td>0.808918</td>\n",
       "      <td>0.038022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>collapse</td>\n",
       "      <td>512</td>\n",
       "      <td>1904</td>\n",
       "      <td>172</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.211921</td>\n",
       "      <td>0.788079</td>\n",
       "      <td>0.071192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>environment</td>\n",
       "      <td>6876</td>\n",
       "      <td>22549</td>\n",
       "      <td>2109</td>\n",
       "      <td>29425</td>\n",
       "      <td>0.233679</td>\n",
       "      <td>0.766321</td>\n",
       "      <td>0.071674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>2046</td>\n",
       "      <td>6026</td>\n",
       "      <td>960</td>\n",
       "      <td>8072</td>\n",
       "      <td>0.253469</td>\n",
       "      <td>0.746531</td>\n",
       "      <td>0.118930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit  gw_counts  cc_counts  authors  total   gw_frac   cc_frac  \\\n",
       "31  climatechange        383       1884      190   2267  0.168946  0.831054   \n",
       "7         climate       2357       9978      469  12335  0.191082  0.808918   \n",
       "27       collapse        512       1904      172   2416  0.211921  0.788079   \n",
       "0     environment       6876      22549     2109  29425  0.233679  0.766321   \n",
       "10      worldnews       2046       6026      960   8072  0.253469  0.746531   \n",
       "\n",
       "    aut_frac  \n",
       "31  0.083811  \n",
       "7   0.038022  \n",
       "27  0.071192  \n",
       "0   0.071674  \n",
       "10  0.118930  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cctop5 = keywords[(keywords[\"total\"] > 2000) & (keywords[\"aut_frac\"] > .01)].sort_values(\"cc_frac\", ascending = False).iloc[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>gw_counts</th>\n",
       "      <th>cc_counts</th>\n",
       "      <th>authors</th>\n",
       "      <th>total</th>\n",
       "      <th>gw_frac</th>\n",
       "      <th>cc_frac</th>\n",
       "      <th>aut_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>1897</td>\n",
       "      <td>1644</td>\n",
       "      <td>1340</td>\n",
       "      <td>3541</td>\n",
       "      <td>0.535724</td>\n",
       "      <td>0.464276</td>\n",
       "      <td>0.378424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>4374</td>\n",
       "      <td>4626</td>\n",
       "      <td>526</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.058444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>1338</td>\n",
       "      <td>1462</td>\n",
       "      <td>512</td>\n",
       "      <td>2800</td>\n",
       "      <td>0.477857</td>\n",
       "      <td>0.522143</td>\n",
       "      <td>0.182857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>askscience</td>\n",
       "      <td>2887</td>\n",
       "      <td>3190</td>\n",
       "      <td>1958</td>\n",
       "      <td>6077</td>\n",
       "      <td>0.475070</td>\n",
       "      <td>0.524930</td>\n",
       "      <td>0.322198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>1090</td>\n",
       "      <td>1226</td>\n",
       "      <td>785</td>\n",
       "      <td>2316</td>\n",
       "      <td>0.470639</td>\n",
       "      <td>0.529361</td>\n",
       "      <td>0.338946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit  gw_counts  cc_counts  authors  total   gw_frac  \\\n",
       "11     Showerthoughts       1897       1644     1340   3541  0.535724   \n",
       "1     climateskeptics       4374       4626      526   9000  0.486000   \n",
       "15         conspiracy       1338       1462      512   2800  0.477857   \n",
       "5          askscience       2887       3190     1958   6077  0.475070   \n",
       "16  explainlikeimfive       1090       1226      785   2316  0.470639   \n",
       "\n",
       "     cc_frac  aut_frac  \n",
       "11  0.464276  0.378424  \n",
       "1   0.514000  0.058444  \n",
       "15  0.522143  0.182857  \n",
       "5   0.524930  0.322198  \n",
       "16  0.529361  0.338946  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gwtop5 = keywords[(keywords[\"total\"] > 2000) & (keywords[\"aut_frac\"] > .01)].sort_values(\"gw_frac\", ascending = False).iloc[:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where do these posts come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>COUNT(domain)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>2980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dailycaller.com</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>washingtonpost.com</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>independent.co.uk</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wattsupwiththat.com</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dailymail.co.uk</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>thinkprogress.org</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>scientificamerican.com</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    domain  COUNT(domain)\n",
       "0              youtube.com           2980\n",
       "4          theguardian.com           1910\n",
       "10         dailycaller.com           1053\n",
       "11             nytimes.com           1024\n",
       "12      washingtonpost.com            939\n",
       "13       independent.co.uk            905\n",
       "15     wattsupwiththat.com            830\n",
       "16         dailymail.co.uk            725\n",
       "17       thinkprogress.org            664\n",
       "18  scientificamerican.com            648"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domaincounts = pd.read_sql(\"\"\"SELECT domain, COUNT(domain) \n",
    "FROM gwtable\n",
    "GROUP BY domain\n",
    "ORDER BY COUNT(domain) DESC\n",
    "\"\"\",\n",
    "               con = engine)\n",
    "\n",
    "news = domaincounts[~(domaincounts[\"domain\"].str.contains(\"self.\")) &\n",
    "             ~(domaincounts[\"domain\"] == \"reddit.com\") & \n",
    "             ~(domaincounts[\"domain\"] == \"imgur.com\") & \n",
    "             ~(domaincounts[\"domain\"] == \"i.imgur.com\") & \n",
    "             ~(domaincounts[\"domain\"] == \"i.redd.it\")]\n",
    "\n",
    "news.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
