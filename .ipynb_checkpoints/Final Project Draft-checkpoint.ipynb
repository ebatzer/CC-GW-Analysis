{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start: \n",
    "- Download files and process them\n",
    "- Must open BZ2 files\n",
    "- Interesting statistics / filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initial Data Processing:__\n",
    "\n",
    "A reddit submission archive is available on \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Text processing\n",
    "import requests # HTML interface\n",
    "import re # Regular expressions\n",
    "import bz2file as bz # Working iwht bz2 files\n",
    "\n",
    "path = \"https://files.pushshift.io/reddit/submissions/\"\n",
    "req = requests.get(path, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(req.text, \"lxml\")\n",
    "tags = soup.findAll(\"a\")\n",
    "\n",
    "urls = []\n",
    "for h in soup.find_all('a'):\n",
    "    link = h.attrs['href']\n",
    "    if link != \"None\":\n",
    "        newlink = re.sub(\"\\\\./\", path, link)\n",
    "        urls.append(newlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://files.pushshift.io/reddit/submissions/RS_2012-03.bz2\n"
     ]
    }
   ],
   "source": [
    "for files in list(set(urls))[3:4]:\n",
    "    print(files)\n",
    "    if \"bz2\" in files:\n",
    "        \n",
    "        # Download that file in a stream\n",
    "        r = requests.get(files, stream = True)\n",
    "        handle = bz.BZ2File('example.bz2', 'wb')\n",
    "        \n",
    "        for chunk in r.iter_content(chunk_size = 512):\n",
    "            if chunk:\n",
    "                handle.write(chunk)\n",
    "            \n",
    "        handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BZ2File' object has no attribute 'decompress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9ecd5e795021>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mshorttext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2Decompressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BZ2File' object has no attribute 'decompress'"
     ]
    }
   ],
   "source": [
    "input_file = bz.BZ2File('example.bz2', 'rb')\n",
    "shorttext = input_file.read(5000)\n",
    "ex = bz.BZ2Decompressor()\n",
    "input_file.decompress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsprocess(f):\n",
    "    \"\"\"\n",
    "    Processes chunks of reddit submission data\n",
    "    \n",
    "    Arguments:\n",
    "        f(chr): BZ2 file path\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        data = f.read(8192)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "for chunk in chunks(f):\n",
    "    process(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'score': 17947, 'title': 'Petition seeks full honors military funeral for hero Florida JROTC student'}\n",
      "{'score': 18980, 'title': 'Eric Lundgren, ‘e-waste’ recycling innovator, faces prison for trying to extend life span of PCs'}\n",
      "{'score': 3040, 'title': 'American Is Charged With Stealing Terra-Cotta Warrior’s Thumb'}\n",
      "{'score': 1816, 'title': \"LIVE: Teens hold 'lie-in' at White House for gun violence\"}\n",
      "{'score': 1846, 'title': 'A $17,850 urine test: Industry boom amid opioid epidemic'}\n",
      "{'score': 9890, 'title': 'At least 4 people shot, including 6-year-old, at Texas Roadhouse in San Antonio'}\n",
      "{'score': 1049, 'title': 'Most KFCs in UK remain closed because of chicken shortage'}\n",
      "{'score': 466, 'title': 'Anthony Borges: 15-year-old boy survives being shot 5 times at Parkland school'}\n",
      "{'score': 20156, 'title': 'Statewide walkout announced for school teachers, employees on Thursday and Friday'}\n",
      "{'score': 364, 'title': 'St. Petersburg set to rename main library after President Obama'}\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='4Z9MRXnkYIH2xw',\n",
    "                     client_secret='3RZSkOcPgtZY87zIL5uNFtXTRs0',\n",
    "                     user_agent='sta141b-proj')\n",
    "\n",
    "print(reddit.read_only)  \n",
    "\n",
    "subreddit = reddit.subreddit('news')\n",
    "\n",
    "for submission in subreddit.hot(limit=10):\n",
    "    print({\"title\":submission.title, \"score\":submission.score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate change sceptics suffer blow as satellite data correction shows 140% faster global warming\n",
      "Who's Still Fighting Climate Change? The U.S. Military: Despite political gridlock over global warming, the Pentagon is pushing ahead with plans to protect its assets from sea-level rise and other impacts.\n",
      "Alaska Village Grapples With Collapse in Walrus Harvest . \"We've heard a lot about global warming and climate change, and we have seen it in the water,\" said Branson Tungiyan, 63, a village elder.\n",
      "Walmart, McDonald’s and 79 Others Commit to Fight Global Warming | A consortium of more than 80 American companies including Walmart, Alcoa and Coca-Cola have agreed to a White House-led plan to combat climate change, the Obama administration said Monday, as the president ramps up his climate plan.\n",
      "Just 90 companies caused two-thirds of man-made global warming emissions Chevron, Exxon and BP among companies most responsible for climate change since dawn of industrial age, figures show\n",
      "Climate change: Fresh doubt over global warming 'pause'\n",
      "The Climate Change Defense? Citing Global Warming, DA Drops Charges Against Anti-Coal Activists who were set to go on trial for blocking a shipment of 40,000 tons of coal.\n",
      "Pentagon to unveil plan for dealing with climate change: Unlike previous Pentagon statements that described climate change as a distant threat, the military is now asserting that global warming \"poses immediate risks to U.S. national security.\"\n",
      "Rick Scott and Florida's environmental dept. denies official ban on 'climate change' and 'global warming'\n",
      "A team of top scientists is telling world leaders to stop congratulating themselves on the Paris agreement to fight climate change because if more isn't done, global temperatures will likely hit dangerous warming levels in about 35 years.\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('news')\n",
    "# sub = subreddit.submissions(1478592000, 1478678400)\n",
    "\n",
    "# for submission in subreddit.submissions(1478592000, 1478678400):\n",
    "#     print(submission.title)\n",
    "    \n",
    "sub = subreddit.search(\"climate chang* | global warm*\", limit = 10, time_filter = \"all\")\n",
    "for submission in sub:\n",
    "    print(submission.title)\n",
    "# for submission in sub.submissions():\n",
    "#     print({\"title\":submission.title, \"score\":submission.score, \"date\":submission.date})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
